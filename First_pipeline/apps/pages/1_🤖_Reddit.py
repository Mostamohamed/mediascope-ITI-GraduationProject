# import streamlit as st
# import redis
# import pandas as pd
# import time
# import altair as alt

# # 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
# st.set_page_config(
#     page_title="Reddit Live Analytics",
#     page_icon="ğŸ¤–",
#     layout="wide"
# )

# st.title("ğŸ¤– Reddit Live: Top Active Communities")
# st.markdown("Real-time aggregation from Kafka & Spark Stream")

# # 2. Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Redis
# try:
#     # Ù„Ø§Ø­Ø¸: Ø¨Ù†Ø³ØªØ®Ø¯Ù… Ø§Ø³Ù… Ø§Ù„ÙƒÙˆÙ†ØªÙŠÙ†Ø± 'redis'
#     r = redis.Redis(host='redis', port=6379, db=0, decode_responses=True)
# except Exception as e:
#     st.error(f"Failed to connect to Redis: {e}")

# placeholder = st.empty()

# def load_data():
#     try:
#         # Ø¨Ù†Ø¬ÙŠØ¨ Ø£Ø¹Ù„Ù‰ 15 ØµØ¨ (Subreddit)
#         # Ø§Ù„Ù…ÙØªØ§Ø­ Ø¯Ù‡ Ù‡Ùˆ Ø§Ù„Ù„ÙŠ Ø§ØªÙÙ‚Ù†Ø§ Ø¹Ù„ÙŠÙ‡ ÙÙŠ ÙƒÙˆØ¯ Spark
#         key = "reddit_subreddits_rank"
#         data = r.zrevrange(key, 0, 15, withscores=True)
        
#         df = pd.DataFrame(data, columns=['Subreddit', 'Total Score'])
        
#         if not df.empty:
#             df['Total Score'] = df['Total Score'].astype(int)
        
#         return df
#     except Exception:
#         return pd.DataFrame(columns=['Subreddit', 'Total Score'])

# # 3. Ø­Ù„Ù‚Ø© Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³ØªÙ…Ø±
# while True:
#     with placeholder.container():
#         df = load_data()

#         # Ù„Ùˆ Ù…ÙÙŠØ´ Ø¯Ø§ØªØ§ (Ù„Ø³Ù‡ Ø§Ù„Ø¨Ø±ÙˆØ¯ÙŠÙˆØ³Ø± Ù…Ø´ØªØºÙ„Ø´)
#         if df.empty:
#             st.info("â³ Waiting for data... (Remember: Reddit Producer runs every 60s)")
#             time.sleep(1)
#             continue

#         # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø´Ø§Ø´Ø©
#         col1, col2 = st.columns([1, 1.5])
        
#         with col1:
#             st.subheader("ğŸ† Leaderboard")
#             # Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ù…Ø¹ Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø±Ø¦ÙŠ
#             st.dataframe(
#                 df,
#                 column_config={
#                     "Subreddit": "Community Name",
#                     "Total Score": st.column_config.ProgressColumn(
#                         "Engagement Score",
#                         format="%d ğŸ”¥",
#                         min_value=0,
#                         # Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§: Ø­Ø·ÙŠÙ†Ø§ int() Ø­ÙˆÙ„ Ø§Ù„Ù‚ÙŠÙ…Ø©
#                         max_value=int(df['Total Score'].max()) if not df.empty else 100
#                     ),
#                 },
#                 use_container_width=True,
#                 hide_index=True
#             )
            
#         with col2:
#             st.subheader("ğŸ“Š Engagement Visualization")
            
#             # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ (Bar Chart)
#             chart = alt.Chart(df).mark_bar(cornerRadius=5).encode(
#                 x=alt.X('Total Score:Q', title='Total Upvotes'),
#                 y=alt.Y('Subreddit:N', sort='-x', title=None),
#                 color=alt.Color('Total Score:Q', scale=alt.Scale(scheme='orangered')),
#                 tooltip=['Subreddit', 'Total Score']
#             ).properties(height=500)
            
#             st.altair_chart(chart, use_container_width=True)
            
#         # Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
#         total_score = df['Total Score'].sum()
#         active_subs = len(df)
        
#         m1, m2 = st.columns(2)
#         m1.metric("Total Analyzed Score", f"{total_score:,}")
#         m2.metric("Active Communities", active_subs)

#     # ØªØ­Ø¯ÙŠØ« ÙƒÙ„ Ø«Ø§Ù†ÙŠØ©
#     time.sleep(2)


# import streamlit as st
# import pandas as pd
# import redis
# import json
# import time

# # ==========================================
# # 1. Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Redis Ù…Ø¨Ø§Ø´Ø±Ø©
# # ==========================================
# # ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù€ host ÙˆØ§Ù„Ù€ port Ù…Ø¸Ø¨ÙˆØ·ÙŠÙ† (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ localhost:6379)
# try:
#     client = redis.Redis(host='redis', port=6379, decode_responses=True)
# except Exception as e:
#     st.error(f"Ù…Ø´ Ø¹Ø§Ø±Ù Ø§ØªØµÙ„ Ø¨Ù€ Redis: {e}")

# st.set_page_config(layout="wide", page_title="Reddit Live")
# st.title("ğŸ”¥ Reddit Real-Time Dashboard (From Redis)")

# # ==========================================
# # 2. Ø¯Ø§Ù„Ø© Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¯Ø§ØªØ§ ÙˆÙÙƒ Ø§Ù„Ù€ JSON
# # ==========================================
# def get_data_from_redis():
#     try:
#         # Ø¨Ù†Ù‚Ø±Ø§ Ø§Ù„Ù€ Hash Ø§Ù„Ù„ÙŠ Ø§Ø³Ù…Ù‡ Ø¸Ø§Ù‡Ø± ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø¹Ù†Ø¯Ùƒ
#         # reddit_subreddits_details
#         data = client.hgetall("reddit_subreddits_details")
        
#         rows = []
#         for subreddit_name, json_str in data.items():
#             try:
#                 # ÙÙƒ Ø§Ù„Ù€ JSON string Ø§Ù„Ù„ÙŠ Ø¬ÙˆÙ‡ Ø§Ù„Ù€ Value
#                 # Ø´ÙƒÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ø¹Ù†Ø¯Ùƒ: {"best_title": "...", "best_score": ...}
#                 details = json.loads(json_str)
                
#                 # Ø¶ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ù€ Subreddit Ù„Ù„Ø¯Ø§ØªØ§
#                 details['subreddit'] = subreddit_name
#                 rows.append(details)
#             except json.JSONDecodeError:
#                 continue
                
#         return pd.DataFrame(rows)
#     except Exception as e:
#         st.error(f"Error reading Redis: {e}")
#         return pd.DataFrame()

# # ==========================================
# # 3. Ø¹Ø±Ø¶ Ø§Ù„Ø¯Ø§ØªØ§
# # ==========================================
# placeholder = st.empty()

# while True:
#     df = get_data_from_redis()
    
#     with placeholder.container():
#         if not df.empty:
#             # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¯Ø§ØªØ§ Ø­Ø³Ø¨ Ø§Ù„Ù€ best_score Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø£Ùˆ Ø³ÙŠØ¨Ù‡Ø§ Ø²ÙŠ Ù…Ø§ Ù‡ÙŠ
#             if 'best_score' in df.columns:
#                 df['best_score'] = df['best_score'].astype(int)
#                 df = df.sort_values(by='best_score', ascending=False)

#             # Ø¹Ø±Ø¶ Ø§Ù„ÙƒØ±ÙˆØª
#             for index, row in df.iterrows():
#                 # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø´Ø§Ù† Ù„Ùˆ ÙÙŠÙ‡ Ø­Ø§Ø¬Ø© Ù†Ø§Ù‚ØµØ© Ù…ØªØ¶Ø±Ø¨Ø´ Ø§ÙŠØ±ÙˆØ±
#                 total_score = row.get('best_score', 0)
#                 sub_name = row.get('subreddit', 'Unknown')
                
#                 high_title = row.get('best_title', 'No Title')
#                 high_url = row.get('best_url', '#')
                
#                 # new_title = row.get('new_title', 'No Title')
#                 # new_url = row.get('new_url', '#')

#                 # Ù„ØºÙŠÙ†Ø§ Ø§Ù„Ù€ columns Ø¹Ø´Ø§Ù† ØªØ§Ø®Ø¯ Ø§Ù„Ø¹Ø±Ø¶ ÙƒÙ„Ù‡
#                 st.success(f"**ğŸ† Highest Voted Post (Score: {total_score:,})**")
#                 st.markdown(f"### {high_title}")
                    
#                 if high_url and high_url != '#':
#                     # Ø²Ø±Ø§Ø± Ø´ÙŠÙƒ ÙŠÙˆØ¯ÙŠÙƒ Ù„Ù„ÙŠÙ†Ùƒ
#                     st.link_button("ğŸ”— View on Reddit", high_url)
                    
#                     # # Newest Post
#                     # with c2:
#                     #     st.success("**ğŸ†• Newest Post**")
#                     #     st.write(f"**Title:** {new_title}")
#                     #     if new_url and new_url != '#':
#                     #         st.markdown(f"[View Post]({new_url})")
#         else:
#             st.warning("âš ï¸ Waiting for data...")
            
#     # ØªØ­Ø¯ÙŠØ« ÙƒÙ„ Ø«Ø§Ù†ÙŠØ©
#     time.sleep(2)
#     # st.rerun() # ÙÙŠ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨ÙŠØ­ØµÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„Ù„ÙˆØ¨ØŒ Ù„Ùˆ Ù…Ø¹Ù„Ù‚ Ø´ÙŠÙ„ Ø§Ù„ÙƒÙˆÙ…Ù†Øª



# import streamlit as st
# import pandas as pd
# import redis
# import json
# import time

# # ==========================================
# # 1. Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Redis
# # ==========================================
# st.set_page_config(layout="wide", page_title="Top Reddit Posts")
# st.title("ğŸ† Highest Voted Post per Subreddit")

# # Ø¶Ø¨Ø· Ø§Ù„Ø§ØªØµØ§Ù„ (ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù‡ÙˆØ³Øª Ø­Ø³Ø¨ Ø¨ÙŠØ¦ØªÙƒ: localhost Ø£Ùˆ redis)
# try:
#     client = redis.Redis(host='redis', port=6379, decode_responses=True)
#     # Ù„Ùˆ Ø´ØºØ§Ù„ Ø¯ÙˆÙƒØ± Ù…Ù…ÙƒÙ† ØªØ­ØªØ§Ø¬: host='redis'
# except Exception as e:
#     st.error(f"Redis Connection Error: {e}")

# # ==========================================
# # 2. Ø¯Ø§Ù„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# # ==========================================
# def get_data():
#     try:
#         # Ø¨Ù†Ø¬ÙŠØ¨ ÙƒÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ù…ØªØ®Ø²Ù†Ø© ÙÙŠ Ø§Ù„Ù‡Ø§Ø´
#         raw_data = client.hgetall("reddit_subreddits_details")
        
#         parsed_rows = []
#         for subreddit, json_str in raw_data.items():
#             try:
#                 # ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± Ù…Ù† JSON Ù„Ù€ Python Dict
#                 details = json.loads(json_str)
                
#                 # Ø¨Ù†Ø±ÙƒØ² Ø¨Ø³ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù€ Best/Highest
#                 row = {
#                     "subreddit": subreddit,
#                     "title": details.get("best_title", "No Title"),
#                     "score": int(details.get("best_score", 0)),
#                     "url": details.get("best_url", "#")
#                 }
#                 parsed_rows.append(row)
#             except:
#                 continue
                
#         return pd.DataFrame(parsed_rows)
#     except Exception as e:
#         return pd.DataFrame()

# # ==========================================
# # 3. Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Highest Post Only)
# # ==========================================
# placeholder = st.empty()

# while True:
#     df = get_data()
    
#     with placeholder.container():
#         if not df.empty:
#             # 1. ØªØ±ØªÙŠØ¨ Ø§Ù„Ù€ Subreddits Ø­Ø³Ø¨ Ø§Ù„Ø³ÙƒÙˆØ± Ù…Ù† Ø§Ù„Ø£Ø¹Ù„Ù‰ Ù„Ù„Ø£Ù‚Ù„
#             df = df.sort_values(by="score", ascending=False)
            
#             # 2. Ø§Ù„Ù„ÙˆØ¨ Ø¯Ù‡ Ù‡Ùˆ Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ¹Ø±Ø¶ "ÙƒÙ„" Ø§Ù„Ù€ Subreddits
#             for index, row in df.iterrows():
                
#                 # ØªØµÙ…ÙŠÙ… Ø§Ù„ÙƒØ§Ø±Øª
#                 with st.expander(f"ğŸ”¥ r/{row['subreddit']} (Top Score: {row['score']:,})", expanded=True):
                    
#                     st.markdown(f"### {row['title']}")
#                     st.caption(f"**Score:** {row['score']:,} â¬†ï¸")
                    
#                     if row['url'] and row['url'] != "#":
#                         st.link_button("ğŸ”— View Post on Reddit", row['url'])
                        
#                         # Ù„Ùˆ Ø§Ù„Ù„ÙŠÙ†Ùƒ ØµÙˆØ±Ø©ØŒ Ø§Ø¹Ø±Ø¶Ù‡Ø§ Ø¬ÙˆÙ‡ Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
#                         if row['url'].endswith(('.jpg', '.png', '.jpeg', '.gif')):
#                             st.image(row['url'], use_container_width=True)

#         else:
#             st.warning("â³ Waiting for data in Redis key: 'reddit_subreddits_details'...")
            
#     time.sleep(2)


import streamlit as st
import pandas as pd
import redis
import json
import time

st.set_page_config(layout="wide", page_title="Reddit Grid View")
st.title("ğŸ† Top Reddit Posts (Grid View)")

# ==========================================
# 1. Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Redis
# ==========================================
try:
    # Ù„Ùˆ Ø§Ù†Øª Docker Ø§Ø³ØªØ®Ø¯Ù… 'redis'ØŒ Ù„Ùˆ local Ø§Ø³ØªØ®Ø¯Ù… 'localhost'
    client = redis.Redis(host='redis', port=6379, decode_responses=True)
except Exception as e:
    st.error(f"Redis Connection Error: {e}")

# ==========================================
# 2. Ø¯Ø§Ù„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ==========================================
def get_data():
    try:
        raw_data = client.hgetall("reddit_subreddits_details")
        parsed_rows = []
        for subreddit, json_str in raw_data.items():
            try:
                details = json.loads(json_str)
                row = {
                    "subreddit": subreddit,
                    "title": details.get("best_title", "No Title"),
                    "score": int(details.get("best_score", 0)),
                    "url": details.get("best_url", "#")
                }
                parsed_rows.append(row)
            except:
                continue
        return pd.DataFrame(parsed_rows)
    except:
        return pd.DataFrame()

# ==========================================
# 3. Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Grid Layout)
# ==========================================
placeholder = st.empty()

while True:
    df = get_data()
    
    with placeholder.container():
        if not df.empty:
            df = df.sort_values(by="score", ascending=False)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ø´Ø§Ù† Ù†Ø¹Ø±Ù Ù†Ù‚Ø³Ù…Ù‡Ø§
            data_list = df.to_dict('records')
            
            # --- Ø§Ù„Ù„ÙˆØ¬ÙŠÙƒ Ø§Ù„Ø¬Ø¯ÙŠØ¯: ÙƒÙ„ Ù„ÙØ© Ø¨Ù†Ø§Ø®Ø¯ Ø¨ÙˆØ³ØªÙŠÙ† ---
            # Ø§Ù„Ø±Ù‚Ù… 2 Ù‡Ù†Ø§ Ù‡Ùˆ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©ØŒ Ù„Ùˆ Ø¹Ø§ÙŠØ² 3 ØºÙŠØ±Ù‡ Ù„Ù€ 3
            COLS_PER_ROW = 1
            
            for i in range(0, len(data_list), COLS_PER_ROW):
                # Ø¨Ù†Ø¹Ù…Ù„ ØµÙ Ø¬Ø¯ÙŠØ¯ ÙÙŠÙ‡ Ø¹Ù…ÙˆØ¯ÙŠÙ†
                cols = st.columns(COLS_PER_ROW)
                
                # Ø¨Ù†Ø§Ø®Ø¯ Ø§Ù„Ø´Ø±ÙŠØ­Ø© (Batch) Ø§Ù„Ù„ÙŠ Ø¹Ù„ÙŠÙ‡Ø§ Ø§Ù„Ø¯ÙˆØ±
                batch = data_list[i : i + COLS_PER_ROW]
                
                # Ø¨Ù†Ù„Ù Ø¬ÙˆÙ‡ Ø§Ù„Ø´Ø±ÙŠØ­Ø© Ø¹Ø´Ø§Ù† Ù†Ø­Ø· ÙƒÙ„ ÙˆØ§Ø­Ø¯ ÙÙŠ Ø¹Ù…ÙˆØ¯Ù‡
                for j, row in enumerate(batch):
                    with cols[j]:
                        # ØªØµÙ…ÙŠÙ… Ø§Ù„ÙƒØ§Ø±Øª
                        with st.expander(f"ğŸ”¥ r/{row['subreddit']}", expanded=True):
                            st.metric("Score", f"{row['score']:,}")
                            st.markdown(f"**{row['title']}**")
                            
                            if row['url'] and row['url'] != "#":
                                st.link_button("ğŸ”— View Post", row['url'])
                                # Ù„Ùˆ Ø¹Ø§ÙŠØ² ØªØ¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø©
                                if row['url'].endswith(('.jpg', '.png', '.jpeg')):
                                    st.image(row['url'], use_container_width=True)
        else:
            st.warning("Waiting for data...")
            
    time.sleep(2)