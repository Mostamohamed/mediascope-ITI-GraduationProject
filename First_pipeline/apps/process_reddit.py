# from pyspark.sql import SparkSession
# from pyspark.sql.functions import from_json, col, sum as _sum
# from pyspark.sql.types import StructType, StringType, LongType, IntegerType, StructField
# import redis

# # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Redis
# REDIS_HOST = 'redis'
# REDIS_PORT = 6379
# REDIS_KEY = "reddit_subreddits_rank"


# def update_redis(df, epoch_id):
#     # Ø¨Ù†Ø­ÙˆÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù€ Pandas
#     pdf = df.toPandas()
    
#     if not pdf.empty:
#         r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)
#         pipe = r.pipeline()
        
#         # Ø¨Ù†ÙØ¶ÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¹Ø´Ø§Ù† Ù†Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© "Ø§Ù„Ù„Ø­Ø¸Ø© Ø¯ÙŠ" Ø¨Ø³
#         # Ù„Ùˆ Ø¹Ø§ÙŠØ² ØªØ±Ø§ÙƒÙ…ÙŠ (ØªØ§Ø±ÙŠØ®ÙŠ) Ø´ÙŠÙ„ Ø§Ù„Ø³Ø·Ø± Ø¯Ù‡
#         r.delete(REDIS_KEY) 
        
#         for index, row in pdf.iterrows():
#             subreddit = row['subreddit']
#             total_score = row['total_score']
            
#             # Ø¨Ù†Ø®Ø²Ù† ÙÙŠ Sorted Set
#             pipe.zadd(REDIS_KEY, {subreddit: total_score})
            
#         pipe.execute()
#         print(f"Batch {epoch_id}: Updated Reddit Rankings for {len(pdf)} subreddits.")

# # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Spark
# spark = SparkSession.builder \
#     .appName("RedditTrendsProcessor") \
#     .master("spark://spark-master:7077") \
#     .config("spark.cores.max", "1") \
#     .getOrCreate()

# spark.sparkContext.setLogLevel("WARN")

# # 2. ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù€ Schema (Ù„Ø§Ø²Ù… ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù€ Producer)
# schema = StructType([
#     StructField("subreddit", StringType()),
#     StructField("score", LongType()),
#     StructField("num_comments", IntegerType()),
#     StructField("title", StringType()),
#     StructField("url", StringType()),
#     StructField("post_timestamp", StringType()),
#     StructField("ingestion_timestamp", StringType())
# ])

# # 3. Ù‚Ø±Ø§Ø¡Ø© Ù…Ù† Kafka
# raw_df = spark.readStream \
#     .format("kafka") \
#     .option("kafka.bootstrap.servers", "kafka:29092") \
#     .option("subscribe", "reddit_data") \
#     .option("startingOffsets", "latest") \
#     .load()

# # 4. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¯Ø§ØªØ§
# parsed_df = raw_df.selectExpr("CAST(value AS STRING)") \
#     .select(from_json(col("value"), schema).alias("data")) \
#     .select("data.*")

# # 5. Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Business Logic)
# # Ù‡Ù†Ø¬Ù…Ø¹ Ø§Ù„Ù€ Score Ù„ÙƒÙ„ Subreddit
# # ÙŠØ¹Ù†ÙŠ Ù„Ùˆ ÙÙŠÙ‡ 5 Ø¨ÙˆØ³ØªØ§Øª Ù…Ù† r/funnyØŒ Ù‡Ù†Ø¬Ù…Ø¹ Ø§Ù„ÙÙˆØªØ³ Ø¨ØªØ§Ø¹ØªÙ‡Ù… ÙƒÙ„Ù‡Ù…
# agg_df = parsed_df.groupBy("subreddit") \
#     .agg(_sum("score").alias("total_score")) \
#     .orderBy(col("total_score").desc())

# # 6. Ø§Ù„ÙƒØªØ§Ø¨Ø© Ù„Ù€ Redis
# query = agg_df.writeStream \
#     .outputMode("complete") \
#     .foreachBatch(update_redis) \
#     .start()

# query.awaitTermination()


# from pyspark.sql import SparkSession
# from pyspark.sql.functions import from_json, col, sum, max, struct
# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
# import os
# import shutil

# # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙÙˆÙ„Ø¯Ø±Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¹Ø´Ø§Ù† Ù†Ø¨Ø¯Ø£ Ø¹Ù„Ù‰ Ù†Ø¸Ø§ÙØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
# # try:
# #     shutil.rmtree("data/reddit_output")
# #     shutil.rmtree("/tmp/reddit_checkpoint")
# # except:
# #     pass

# spark = SparkSession.builder \
#     .appName("RedditAggregatorBackend") \
#     .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
#     .getOrCreate()

# spark.sparkContext.setLogLevel("ERROR")

# # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø³ÙƒÙŠÙ…Ø§
# schema = StructType([
#     StructField("subreddit", StringType(), True),
#     StructField("title", StringType(), True),
#     StructField("score", IntegerType(), True),
#     StructField("url", StringType(), True),
#     StructField("created_utc", DoubleType(), True)
# ])

# # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù† Kafka
# df_kafka = spark.readStream \
#     .format("kafka") \
#     .option("kafka.bootstrap.servers", "localhost:9092") \
#     .option("subscribe", "reddit_data") \
#     .option("startingOffsets", "earliest") \
#     .load()

# df_parsed = df_kafka.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

# # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Aggregation)
# aggregated_df = df_parsed.groupBy("subreddit").agg(
#     sum("score").alias("total_score"),
#     max(struct(col("score"), col("title"), col("url"))).alias("top_post"),
#     max(struct(col("created_utc"), col("title"), col("url"))).alias("newest_post")
# )

# final_df = aggregated_df.select(
#     col("subreddit"),
#     col("total_score"),
#     col("top_post.title").alias("high_title"),
#     col("top_post.score").alias("high_score"),
#     col("top_post.url").alias("high_url"),
#     col("newest_post.title").alias("new_title"),
#     col("newest_post.url").alias("new_url")
# )

# # --- Ø¯Ø§Ù„Ø© Ù„Ù„ÙƒØªØ§Ø¨Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Overwrite ---
# def write_batch_to_parquet(batch_df, batch_id):
#     # Ø¨Ù†ÙƒØªØ¨ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙˆÙ†Ù…Ø³Ø­ Ø§Ù„Ù‚Ø¯ÙŠÙ… (Overwrite)
#     # Ø¯Ù‡ Ø¨ÙŠØ®Ù„ÙŠ Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø´Ø§ÙŠÙØ© Ø£Ø­Ø¯Ø« Ø£Ø±Ù‚Ø§Ù… Ø¨Ø³
#     batch_df.write \
#         .mode("overwrite") \
#         .parquet("data/reddit_output")

# print("ğŸš€ Spark Job Started... Writing live updates to data/reddit_output")

# # Ø§Ø³ØªØ®Ø¯Ø§Ù… foreachBatch Ù‡Ùˆ Ø§Ù„Ø­Ù„
# query = final_df.writeStream \
#     .outputMode("complete") \
#     .foreachBatch(write_batch_to_parquet) \
#     .option("checkpointLocation", "/tmp/reddit_checkpoint_v2") \
#     .trigger(processingTime="5 seconds") \
#     .start()

# query.awaitTermination()
# from pyspark.sql import SparkSession
# from pyspark.sql.functions import from_json, col, sum as _sum
# from pyspark.sql.types import StructType, StringType, LongType, IntegerType, StructField
# import redis

# # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Redis
# REDIS_HOST = 'redis'
# REDIS_PORT = 6379
# REDIS_KEY = "reddit_subreddits_rank"

# def update_redis(df, epoch_id):
#     pdf = df.toPandas()
#     if not pdf.empty:
#         try:
#             r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)
#             pipe = r.pipeline()
#             # r.delete(REDIS_KEY) # Ø§Ø®ØªÙŠØ§Ø±ÙŠ: Ù„Ùˆ Ø¹Ø§ÙŠØ² ØªÙ…Ø³Ø­ Ø§Ù„Ù‚Ø¯ÙŠÙ… ÙƒÙ„ Ù…Ø±Ø©
#             for index, row in pdf.iterrows():
#                 pipe.zadd(REDIS_KEY, {row['subreddit']: row['total_score']})
#             pipe.execute()
#             print(f"âœ… Batch {epoch_id}: Updated Redis with {len(pdf)} subreddits.")
#         except Exception as e:
#             print(f"âŒ Redis Error: {e}")

# spark = SparkSession.builder \
#     .appName("RedditTrendsProcessor") \
#     .master("spark://spark-master:7077") \
#     .config("spark.cores.max", "1") \
#     .getOrCreate()

# spark.sparkContext.setLogLevel("WARN")

# # Ù†ÙØ³ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ù„ÙŠ ÙÙŠ Producer
# schema = StructType([
#     StructField("subreddit", StringType()),
#     StructField("score", LongType()),
#     StructField("num_comments", IntegerType()),
#     StructField("title", StringType()),
#     StructField("url", StringType()),
#     StructField("post_timestamp", StringType()),
#     StructField("ingestion_timestamp", StringType())
# ])

# # Ù‡Ù†Ø§ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù…Ù‡Ù…: kafka:29092 ÙˆØ§Ø³Ù… Ø§Ù„ØªÙˆØ¨ÙŠÙƒ reddit_data
# raw_df = spark.readStream \
#     .format("kafka") \
#     .option("kafka.bootstrap.servers", "kafka:29092") \
#     .option("subscribe", "reddit_data") \
#     .option("startingOffsets", "earliest") \
#     .load()

# parsed_df = raw_df.selectExpr("CAST(value AS STRING)") \
#     .select(from_json(col("value"), schema).alias("data")) \
#     .select("data.*")

# agg_df = parsed_df.groupBy("subreddit") \
#     .agg(_sum("score").alias("total_score")) \
#     .orderBy(col("total_score").desc())

# query = agg_df.writeStream \
#     .outputMode("complete") \
#     .foreachBatch(update_redis) \
#     .start()

# query.awaitTermination()


from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, max, struct, to_json
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
import redis

# ==========================================
# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„
# ==========================================
# Ù„Ùˆ Ø´ØºØ§Ù„ DockerØŒ Ø§Ø³ØªØ®Ø¯Ù… "kafka:29092" Ùˆ "redis"
# Ù„Ùˆ Ø´ØºØ§Ù„ LocalØŒ Ø§Ø³ØªØ®Ø¯Ù… "localhost:9092" Ùˆ "localhost"
KAFKA_BOOTSTRAP_SERVERS = "kafka:29092" 
REDIS_HOST = "redis"
REDIS_PORT = 6379

# ==========================================
# 2. ØªØ´ØºÙŠÙ„ Spark Session
# ==========================================
spark = SparkSession.builder \
    .appName("RedditSparkProcessor") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

# ==========================================
# 3. ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø³ÙƒÙŠÙ…Ø§ (Ù†ÙØ³ Ø´ÙƒÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ù„ÙŠ Ø¬Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆØ¯ÙŠÙˆØ³Ø±)
# ==========================================
# Ø§Ù„Ø¨Ø±ÙˆØ¯ÙŠÙˆØ³Ø± Ø¨ÙŠØ¨Ø¹Øª: subreddit, score, num_comments, title, url, post_timestamp
schema = StructType([
    StructField("subreddit", StringType(), True),
    StructField("score", IntegerType(), True),
    StructField("num_comments", IntegerType(), True),
    StructField("title", StringType(), True),
    StructField("url", StringType(), True),
    StructField("post_timestamp", StringType(), True),
    StructField("ingestion_timestamp", StringType(), True)
])

# ==========================================
# 4. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¯Ø§ØªØ§ Ù…Ù† Kafka
# ==========================================
df_raw = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
    .option("subscribe", "reddit_data") \
    .option("startingOffsets", "earliest") \
    .load()

# ÙÙƒ Ø§Ù„Ù€ JSON ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ø£Ø¹Ù…Ø¯Ø©
df_parsed = df_raw.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

# ==========================================
# 5. Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ)
# ==========================================
# Ø¹Ø§ÙŠØ²ÙŠÙ† Ù„ÙƒÙ„ subreddit Ù†Ø·Ù„Ø¹ Ø­Ø§Ø¬ØªÙŠÙ†:
# 1. Ø£Ø¹Ù„Ù‰ Ø¨ÙˆØ³Øª (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ score)
# 2. Ø£Ø¬Ø¯Ø¯ Ø¨ÙˆØ³Øª (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ post_timestamp)

aggregated_df = df_parsed.groupBy("subreddit").agg(
    # Ø¨Ù†Ø¹Ù…Ù„ struct ÙŠØ±Ø¨Ø· Ø§Ù„Ø³ÙƒÙˆØ± Ø¨Ø¨Ø§Ù‚ÙŠ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø¹Ø´Ø§Ù† Ù„Ù…Ø§ Ù†Ø§Ø®Ø¯ Ø§Ù„Ù…Ø§ÙƒØ³ Ù†Ø§Ø®Ø¯Ù‡Ù… Ù…Ø¹Ø§Ù‡
    max(struct(col("score"), col("title"), col("url"))).alias("best_post_data"),
    
    # ÙˆÙ†ÙØ³ Ø§Ù„ÙƒÙ„Ø§Ù… Ù„Ù„ÙˆÙ‚Øª
    max(struct(col("post_timestamp"), col("title"), col("url"))).alias("new_post_data")
)

# ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù€ Redis (JSON String)
# Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ù…Ø³ØªÙ†ÙŠØ© Ù…ÙØ§ØªÙŠØ­ Ø§Ø³Ù…Ù‡Ø§: best_title, best_score, best_url
final_df = aggregated_df.select(
    col("subreddit"),
    to_json(struct(
        col("best_post_data.title").alias("best_title"),
        col("best_post_data.score").alias("best_score"),
        col("best_post_data.url").alias("best_url"),
        # col("new_post_data.title").alias("new_title"),
        # col("new_post_data.url").alias("new_url")
    )).alias("json_value") # Ø¯Ù‡ Ø§Ù„Ù„ÙŠ Ù‡ÙŠØªØ®Ø²Ù† Ø¬ÙˆÙ‡ Ø§Ù„Ø±ÙŠØ¯ÙŠØ³
)

# ==========================================
# 6. Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ Redis
# ==========================================
def write_to_redis(batch_df, batch_id):
    # Ø¨Ù†Ø­ÙˆÙ„ Ø§Ù„Ø¨Ø§ØªØ´ Ù„Ù€ Pandas Ø¹Ø´Ø§Ù† Ù†ÙƒØªØ¨Ù‡ Ø¨Ø³Ø±Ø¹Ø© ÙÙŠ Redis (Ø£Ø³Ù‡Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù€ Hashes)
    # Ù…Ù„Ø§Ø­Ø¸Ø©: Ù„Ùˆ Ø§Ù„Ø¯Ø§ØªØ§ Ø¶Ø®Ù…Ø© Ø¬Ø¯Ø§Ù‹ ÙŠÙØ¶Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… foreachPartitionØŒ Ø¨Ø³ Ù‡Ù†Ø§ Ø§Ù„Ø¯Ø§ØªØ§ Ù…Ù„Ù…ÙˆÙ…Ø©
    data = batch_df.collect()
    
    if data:
        try:
            # ÙØªØ­ Ø§ØªØµØ§Ù„ Ù…Ø¹ Redis
            r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
            
            # Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø¯Ø§Ø®Ù„ Pipeline Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
            pipe = r.pipeline()
            
            for row in data:
                # Key: reddit_subreddits_details
                # Field: Ø§Ø³Ù… Ø§Ù„Ù€ Subreddit
                # Value: Ø§Ù„Ù€ JSON Ø§Ù„Ù„ÙŠ Ø¬Ù‡Ø²Ù†Ø§Ù‡
                pipe.hset("reddit_subreddits_details", row['subreddit'], row['json_value'])
            
            pipe.execute()
            print(f"âœ… Batch {batch_id}: Updated {len(data)} subreddits in Redis.")
        except Exception as e:
            print(f"âŒ Error writing to Redis: {e}")

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ù€ Query
query = final_df.writeStream \
    .outputMode("update") \
    .foreachBatch(write_to_redis) \
    .trigger(processingTime="10 seconds") \
    .start()

query.awaitTermination()