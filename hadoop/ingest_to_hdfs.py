import json
import time
import uuid  # Ù…ÙƒØªØ¨Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±ÙØ§Øª ÙØ±ÙŠØ¯Ø© Ù„Ù…Ù†Ø¹ ØªÙƒØ±Ø§Ø± Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª
from datetime import datetime
from kafka import KafkaConsumer
from hdfs import InsecureClient
from collections import defaultdict

# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„
# Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ù†ÙØ° 29092 Ù„Ø£Ù†Ù‡ Ø§Ù„Ù…Ù†ÙØ° Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¯ÙˆÙƒØ±
KAFKA_SERVER = 'kafka:29092'
HDFS_URL = 'http://namenode:9870'
HDFS_USER = 'root'

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø³Ø­Ø¨Ù‡Ø§
TOPICS = ['reddit_data', 'youtube_data', 'twitch_streams', 'tiktok_data']

def get_hdfs_client():
    print("â³ Connecting to HDFS...")
    while True:
        try:
            client = InsecureClient(HDFS_URL, user=HDFS_USER)
            client.list('/') # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ Ù„Ù„Ø§ØªØµØ§Ù„
            print("âœ… Connected to HDFS NameNode!")
            return client
        except Exception as e:
            print(f"âš ï¸ HDFS not ready... retrying. ({e})")
            time.sleep(5)

def save_to_hdfs(client, topic, data_list):
    if not data_list: return
    try:
        now = datetime.now()
        # Ù…Ø³Ø§Ø± Ø§Ù„ØªØ®Ø²ÙŠÙ†: ÙŠØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªÙˆØ¨ÙŠÙƒ Ø«Ù… Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø³Ù†Ø©/Ø´Ù‡Ø±/ÙŠÙˆÙ…)
        # Ù…Ø«Ø§Ù„: /datalake/youtube_data/2025/12/04
        folder_path = f"/datalake/{topic}/{now.year}/{now.month:02d}/{now.day:02d}"
        
        # ==================================================================
        # Ø§Ù„Ø­Ù„ Ø§Ù„Ø¬Ø°Ø±ÙŠ Ù„Ù…Ø´ÙƒÙ„Ø© (File already exists)
        # Ù†Ø¶ÙŠÙ ÙƒÙˆØ¯ Ø¹Ø´ÙˆØ§Ø¦ÙŠ (UUID) Ù„Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø±Ù‡ Ø£Ø¨Ø¯Ø§Ù‹
        # ==================================================================
        unique_id = uuid.uuid4().hex[:8]
        file_name = f"{topic}_{int(time.time())}_{unique_id}.json"
        
        full_path = f"{folder_path}/{file_name}"
        
        # ØªØ­ÙˆÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù†Øµ JSON (ÙƒÙ„ ØµÙ ÙÙŠ Ø³Ø·Ø± Ù…Ù†ÙØµÙ„)
        json_content = "\n".join([json.dumps(record) for record in data_list])
        
        # Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ© ÙÙŠ HDFS
        with client.write(full_path, encoding='utf-8') as writer:
            writer.write(json_content)
            
        print(f"ðŸ’¾ [HDFS] Saved {len(data_list)} records to: {full_path}")
        
    except Exception as e:
        print(f"âŒ Write Error: {e}")

def run():
    print("ðŸš€ Starting Permanent Data Warehouse Ingestion...")
    
    # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø³ÙŠØ· Ù„Ø¶Ù…Ø§Ù† Ø£Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ ØªØ¹Ù…Ù„
    time.sleep(5)
    
    hdfs_client = get_hdfs_client()
    
    print(f"ðŸŽ§ Connecting to Kafka at{KAFKA_SERVER}...")
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³ØªÙ‡Ù„Ùƒ (Consumer)
    consumer = KafkaConsumer(
        *TOPICS,
        bootstrap_servers=[KAFKA_SERVER],
        # 1. Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Earliest) Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø³Ø¬Ù„ Ø³Ø§Ø¨Ù‚
        auto_offset_reset='earliest', 
        # 2. ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø­ÙØ¸ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø°ÙŠ ØªÙˆÙ‚ÙÙ†Ø§ Ø¹Ù†Ø¯Ù‡
        enable_auto_commit=True, 
        value_deserializer=lambda x: json.loads(x.decode('utf-8')),
        # 3. Ø§Ø³Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¬Ø¯ÙŠØ¯ Ù„Ø¶Ù…Ø§Ù† Ø³Ø­Ø¨ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        group_id='social_pulse_warehouse_final_v6' 
    )

    # Ù…Ø®Ø²Ù† Ù…Ø¤Ù‚Øª (Buffer) Ù„ÙƒÙ„ ØªÙˆØ¨ÙŠÙƒ Ø¹Ù„Ù‰ Ø­Ø¯Ø©
    buffers = defaultdict(list)
    
    # Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©: ÙŠØªÙ… Ø§Ù„Ø­ÙØ¸ ÙƒÙ„ 10 Ø±Ø³Ø§Ø¦Ù„ (Ù„ØªØ­Ù‚ÙŠÙ‚ ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡)
    BATCH_SIZE = 10 
    
    print(f"ðŸŽ§ Connected to Kafka. Monitoring Topics: {TOPICS}")
    
    for message in consumer:
        data = message.value
        topic = message.topic
        
        # Ø¥Ø¶Ø§ÙØ© ØªÙˆÙ‚ÙŠØª Ø§Ù„Ø£Ø±Ø´ÙØ© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data['ingested_at'] = datetime.now().isoformat()
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù„Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„ØªÙˆØ¨ÙŠÙƒ
        buffers[topic].append(data)
        print(f"ðŸ“¥ [{topic}] received msg...")
        # Ø¥Ø°Ø§ Ø§Ù…ØªÙ„Ø£ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù‡Ø°Ø§ Ø§Ù„ØªÙˆØ¨ÙŠÙƒØŒ Ù‚Ù… Ø¨Ø§Ù„Ø­ÙØ¸ ÙˆØªÙØ±ÙŠØºÙ‡
        if len(buffers[topic]) >= BATCH_SIZE:
            save_to_hdfs(hdfs_client, topic, buffers[topic])
            buffers[topic] = [] 

if __name__ == "__main__":
    run()