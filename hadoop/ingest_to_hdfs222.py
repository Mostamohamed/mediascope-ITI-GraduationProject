import json
import time
from datetime import datetime
from kafka import KafkaConsumer
from hdfs import InsecureClient
from collections import defaultdict
import uuid

# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„
KAFKA_SERVER = 'kafka:29092'
HDFS_URL = 'http://namenode:9870'
HDFS_USER = 'root'
TOPICS = ['reddit_data', 'youtube_data', 'twitch_streams', 'tiktok_data']

def get_hdfs_client():
    print("â³ Connecting to HDFS...")
    while True:
        try:
            client = InsecureClient(HDFS_URL, user=HDFS_USER)
            client.list('/') 
            print("âœ… Connected to HDFS NameNode!")
            return client
        except Exception as e:
            print(f"âš ï¸ HDFS not ready... retrying. ({e})")
            time.sleep(5)

def save_to_hdfs(client, topic, data_list):
    if not data_list: return
    try:
        now = datetime.now()
        # Ù…Ø³Ø§Ø± Ø§Ù„ØªØ®Ø²ÙŠÙ†: Ø§Ø³Ù… Ø§Ù„ØªÙˆØ¨ÙŠÙƒ / Ø§Ù„Ø³Ù†Ø© / Ø§Ù„Ø´Ù‡Ø± / Ø§Ù„ÙŠÙˆÙ…
        folder_path = f"/datalake/{topic}/{now.year}/{now.month:02d}/{now.day:02d}"
        
        # Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù ÙØ±ÙŠØ¯ Ø¨Ø§Ù„ÙˆÙ‚Øª
        
        file_name = f"{topic}_data_{int(time.time())}_{len(data_list)}.json"
        full_path = f"{folder_path}/{file_name}"
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ù„Ù†Øµ
        json_content = "\n".join([json.dumps(record) for record in data_list])
        
        with client.write(full_path, encoding='utf-8') as writer:
            writer.write(json_content)
            
        print(f"ğŸ’¾ [HDFS] Saved {len(data_list)} records to: {full_path}")
        
    except Exception as e:
        print(f"âŒ Write Error: {e}")

def run():
    print("ğŸš€ Starting Permanent Data Warehouse Ingestion...")
    time.sleep(5)
    
    hdfs_client = get_hdfs_client()
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³ØªÙ‡Ù„Ùƒ
    consumer = KafkaConsumer(
        *TOPICS,
        bootstrap_servers=[KAFKA_SERVER],
        # 1. Ù‡Ø§Øª Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø®Ø§Ù„Øµ Ù„Ùˆ Ù…Ø¹Ù†Ø¯ÙƒØ´ Ù…Ø±Ø¬Ø¹ÙŠØ©
        auto_offset_reset='earliest', 
        # 2. Ø§Ø­ÙØ¸ Ù…ÙƒØ§Ù†Ùƒ Ø¹Ø´Ø§Ù† Ù„Ùˆ Ø§Ù„ÙƒÙˆÙ†ØªÙŠÙ†Ø± Ø±Ø³ØªØ± Ù…ÙŠØ³Ø­Ø¨Ø´ Ø§Ù„Ù‚Ø¯ÙŠÙ… ØªØ§Ù†ÙŠ
        enable_auto_commit=True, 
        value_deserializer=lambda x: json.loads(x.decode('utf-8')),
        # 3. Ø§Ø³Ù… Ø¬Ø±ÙˆØ¨ Ø«Ø§Ø¨Øª Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ (Ø¹Ø´Ø§Ù† ÙŠÙƒÙ…Ù„ Ù…ÙƒØ§Ù† Ù…Ø§ ÙˆÙ‚Ù)
        # ØºÙŠØ± Ø§Ù„Ø§Ø³Ù… Ø¯Ù‡ Ù„Ùˆ Ø¹Ø§ÙŠØ² ØªØ³Ø­Ø¨ Ø§Ù„Ù‡ÙŠØ³ØªÙˆØ±ÙŠ ÙƒÙ„Ù‡ Ù…Ù† Ø§Ù„Ø£ÙˆÙ„ Ø¯Ù„ÙˆÙ‚ØªÙŠ Ø­Ø§Ù„Ø§
        group_id='social_pulse_warehouse_final_v1' 
    )

    buffers = defaultdict(list)
    # ÙƒÙ„ 10 Ø±Ø³Ø§ÙŠÙ„ ÙŠÙƒØªØ¨Ù‡Ù… (Ø±Ù‚Ù… Ù…ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡)
    # Ù„Ùˆ Ø¹Ø§ÙŠØ² ØªØ´ÙˆÙÙ‡Ù… ÙÙˆØ±Ø§Ù‹ Ø®Ù„ÙŠÙ‡Ø§ 1
    BATCH_SIZE = 10 
    
    print(f"ğŸ§ Connected to Kafka. Monitoring Topics: {TOPICS}")
    
    for message in consumer:
        data = message.value
        topic = message.topic
        
        # Ø¥Ø¶Ø§ÙØ© ÙˆÙ‚Øª Ø§Ù„Ø£Ø±Ø´ÙØ©
        data['ingested_at'] = datetime.now().isoformat()
        
        buffers[topic].append(data)
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø¹Ø´Ø§Ù† Ù†Ø·Ù…Ù†
        print(f"ğŸ“¥ [{topic}] received msg...")

        if len(buffers[topic]) >= BATCH_SIZE:
            save_to_hdfs(hdfs_client, topic, buffers[topic])
            buffers[topic] = [] # ÙØ¶ÙŠ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚

if __name__ == "__main__":
    run()